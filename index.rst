..
  Technote content.

  See https://developer.lsst.io/restructuredtext/style.html
  for a guide to reStructuredText writing.

  Do not put the title, authors or other metadata in this document;
  those are automatically added.

  Use the following syntax for sections:

  Sections
  ========

  and

  Subsections
  -----------

  and

  Subsubsections
  ^^^^^^^^^^^^^^

  To add images, add the image file (png, svg or jpeg preferred) to the
  _static/ directory. The reST syntax for adding the image is

  .. figure:: /_static/filename.ext
     :name: fig-label

     Caption text.

   Run: ``make html`` and ``open _build/html/index.html`` to preview your work.
   See the README at https://github.com/lsst-sqre/lsst-technote-bootstrap or
   this repo's README for more info.

   Feel free to delete this instructional comment.

:tocdepth: 1

.. Please do not modify tocdepth; will be fixed when a new Sphinx theme is shipped.

.. sectnum::

.. TODO: Delete the note below before merging new content to the master branch.

.. note::

   **This technote is not yet published.**

   Having every job in a workflow read from registry and datastore and then write to registry and datastore does not scale when millions of jobs are involved.
   Instead we need an approach where the data are retrieved at the start of the workflow and stored at the end of the workflow, but during the workflow execution only local files are in use.
   This note describes the issues and provides a possible design.

Introduction
============

When pipelines are executed by invoking the ``pipetask`` command, each ``PipelineTask`` that is executed results in calls to ``Butler.get()`` at the start to retrieve the relevant data and calls to ``Butler.put()`` at the end to write the results to Butler registry.
In a workflow environment each job being executed currently uses a shared PostgreSQL registry and a remote object store.
The general scheme is described in :cite:`DMTN-157` in the context of the Google Proof of Concept.
This approach works well until thousands of jobs are running simultaneously and they all need to connect to the database at the same time.
It's also made worse if jobs take roughly the same amount of time to execute and they are all starting at the same time and completing at the same time.
We mitigated some of this problem in the Google Proof of Concept by spreading the start time of the initial jobs randomly over a ten minute period.
This smoothed out some of the spikes and allowed us to execute a few thousand jobs.

Unfortunately this approach will not scale when hundreds of thousands to millions of jobs are in flight at the same time and all trying to write to the same PostgresSQL server.
It is also incompatible with a distributed processing approach where multiple data facilities are processing a fraction of the data and do not want to be reliant on one single server.
We therefore need to change how we manage access to the registry and datastores when we execute large workflows.

Shared-Nothing
==============

The best way to scale processing is for each job to be fully isolated in that it only uses files provided locally to it by the workflow system and it only writes out data to local files that will be gathered by the workflow system and passed to the next job.

The butler middleware does provide a solution to this problem in that it supports different database backends and different datastore backends.
A butler can be configured to use a local SQLite file as the registry and a local file system as a datastore.

The way this could work is if the workflow generator inserts an extra job at the start of the workflow.
This job would query registry and download all the datasets to local files along with all the relevant registry information (done as a ``butler export``).
The registry information and data files would then be loaded into a SQLite database.
The SQLite file and data files would then be passed to the relevant jobs with each job receiving a YAML export file from the SQLite registry and the requested subset of data files.
The registry information generated by each job would have a unique name to ensure that multiple jobs can be combined in later ones.

Each job would then consist of three phases.

1. Take each of the input YAML registry files (multiple jobs can be inputs to a job) and combine them into a single SQLite registry suitable for use by ``pipetask``.

2. Execute the pipeline tasks using the local registry and writing to local files.

3. Export the registry information from the SQLite into YAML, giving it a unique name.
   This YAML file would then be sent to the next job along with the new files created by this job.

Once the workflow completes the situation will be that there will be a number of exported registry YAML files containing the full provenance of their part of the network, and data files created by each job as intermediate files.

Before dependent workflows can be started these data files and registry information must be synched back to the main registry.
This could either be done as a rate-limited queue where access to the registry is restricted to a certain number of simultaneous writes, or it could be a special job that automatically runs at the end of the workflow (assuming intermediate files are available to the process).

Questions
---------

* Should there be a single job at the start of the workflow that pulls down all the data or should each of the initial jobs (such as single-frame processing) have its own small job inserted into the graph that downloads just the files it needs (calibrations and the single file)?
* If many workflows are submitted simultaneously is there still contention if each workflow kicks off by hitting the database?
* Should there be an independent staging job that runs outside of the workflow downloading all the data in advance?
* If the graph has multiple independent final nodes should they be followed by their own upload job or should the workflow block until all jobs have completed before doing a single write connection to the Butler? (the single write is better for load balancing).
* Can a single ingest job running at the end of the workflow see all the intermediate data files in order to upload them?

Challenges
==========

Conceptually the approach of downloading all the information from the main repository at the start and then finally storing everything on completion is fairly straightforward.
Complications arise in the subtleties of merging content from multiple temporary registries.
Butler does not yet robustly handle the merging of exported registry files into a single registry when duplicate information is present.
Dimensions will be defined during the initial export from the parent registry and duplicated in the export files and there can also be shared provenance present from intermediate files.

The BPS system would need to be modified to optionally add the initialize job and to ensure that the command executed for each job has the code for registry merging and export.
It is probably not necessary to add this support directly to the ``pipetask`` command.

The assumption is that the workflow jobs are executed using generic service account permissions for accessing the registry and datastore.
Some checks would be required to ensure that the user submitting the job has permission to read from the input collections and write to the output collection.
The alternative is to run the jobs as the science platform user with butler access mediated through the client/server architecture described in :cite:`DMTN-176`.

.. note::
  Could rate-limiting registry access be mediated by the Registry REST server?

.. Add content here.
.. Do not include the document title (it's automatically added from metadata.yaml).

.. rubric:: References

.. Make in-text citations with: :cite:`bibkey`.

.. bibliography:: local.bib lsstbib/books.bib lsstbib/lsst.bib lsstbib/lsst-dm.bib lsstbib/refs.bib lsstbib/refs_ads.bib
    :style: lsst_aa
